#!/usr/bin/bash

set -e

ACTION=$1
RESOURCE=$2
VERSION=1.1
HANDLER_NAME="soulsgen"
MODEL_PATH="../model.pth"  # Model file must be relative for archiver
MODEL_STORE="$PWD/serve-handler"

ZONE=us-central1-a
INSTANCE=darksouls-trainer

UPLOAD_LOCATION="$BUCKET/models/soulsgen/v$VERSION"

EXPANDA_CONFIG="dataset/expanda.cfg"
EXPANDA_BASE_CONFIG="dataset/expanda.base.cfg"

function download_model() {
  if [ ! -f serve-handler/model.pth ]; then
    echo "Model Checkpoint Not Found! Downloaing..."
    gsutil cp "$UPLOAD_LOCATION/model.pth" "serve-handler"
  fi
}

# Upload related torchserve assets, making their permisions ridiculously
# open and copying POSIX file attrs so it can be used by the application
function upload_handler_archive() {
  chmod 777 "serve-handler/$HANDLER_NAME.mar"
  chmod 777 serve-handler/assets/*
  gsutil cp -P "serve-handler/$HANDLER_NAME.mar" "$BUCKET/torchserve-assets/model-store"
  gsutil cp -P serve-handler/assets/* "$BUCKET/torchserve-assets/assets/soulsgen/corpus"
}

function download_handler_archive() {
  gsutil cp -P "$BUCKET/torchserve-assets/model-store/$HANDLER_NAME.mar" serve-handler
  gsutil cp -P "$BUCKET/torchserve-assets/assets/soulsgen/corpus/*" serve-handler/assets
}

# Create a model archive for torchserve
# Automatically takes required files for an archive
# Will include any *.py files as extra files (exludes model, handler or test files)
# and includes a requirments file as a dependency
function archive() {
  ARCHIVER_ARGS="-f --model-name $HANDLER_NAME --version $VERSION --serialized-file $MODEL_PATH --model-file model.py --handler handler.py --export-path $MODEL_STORE"
  if [ -f "requirements.txt" ]; then
      ARCHIVER_ARGS="$ARCHIVER_ARGS --requirements-file requirements.txt"
  fi
  FILES="$(find . -type f -name '*.py' -not -name model.py -not -name test.py -not -name handler.py -not -name requirements.txt | cut -d/ -f2-)"
  ARCHIVER_ARGS="$ARCHIVER_ARGS --extra-files "
  for file in $FILES; do
      ARCHIVER_ARGS="$ARCHIVER_ARGS$file,"
  done
  poetry run torch-model-archiver ${ARCHIVER_ARGS%?}  # Remove trailing comma
}

# A sort of hack for automating the dataset build
# Appends the file param args to a skeleton config file into
# the actual config file, this way we don't have to manually put in each file
# but rather any xml file in `DATASOURCE_DIR` will be pulled
function build_expanda_config() {
  if [ -f $EXPANDA_CONFIG ]; then
    rm $EXPANDA_CONFIG
  fi
  cp $EXPANDA_BASE_CONFIG $EXPANDA_CONFIG
  FILES="$(find $DATASOURCE_DIR -type f -name '*.xml')"
  for file in $FILES; do
      echo "    --soulsgen.parse $file" >> $EXPANDA_CONFIG
  done
}

# Build our data set and distribute the artifacts accordingly
function build_corpus() {
  build_expanda_config
  (cd dataset && poetry run expanda build)
  cp dataset/build/vocab.txt serve-handler/assets/vocab.txt
  poetry run python -m soulsgen.inspect \
    --source_text dataset/build/corpus.raw.txt \
    --output_path  serve-handler/assets/corpus.txt \
    --words-export
  poetry run python -m soulsgen.inspect \
    --source_text dataset/build/corpus.raw.txt \
    --output_path  dataset/build/report.html
}

function build_sentence_report() {
  poetry run python -m soulsgen.generate \
    --source-size $(wc -l < dataset/build/corpus.raw.txt) \
    --output-file  generated.txt \
    --host $TORCHSERVE_ENDPOINT
  poetry run python -m soulsgen.inspect \
    --source_text generated.txt \
    --output_path report.html
}

function upload_corpus() {
  zip -r corpus.zip dataset/build
  gsutil cp corpus.zip $UPLOAD_LOCATION
  rm corpus.zip
}

function download_corpus() {
  gsutil cp "$UPLOAD_LOCATION/corpus.zip" .
  unzip corpus.zip
  rm corpus.zip
}

# Create a GCP Compute instance with an available GPU
# The instance will automatically install the trainer and dataset
# Takes some short time for ssh to become available
function deploy_trainer() {
  echo "#!/bin/bash" > run_trainer.sh
  envsubst < trainer_script.sh | sed '/^[[:blank:]]*#/d;s/#.*//' >> run_trainer.sh
  gcloud compute instances create $INSTANCE \
      --zone=$ZONE \
      --image-family=pytorch-latest-gpu \
      --image-project=deeplearning-platform-release \
      --machine-type=n1-highmem-2 \
      --boot-disk-size=50GB \
      --accelerator="type=nvidia-tesla-t4,count=1" \
      --metadata="install-nvidia-driver=True" \
      --maintenance-policy=TERMINATE \
      --scopes=default,storage-rw \
      --metadata-from-file=startup-script=run_trainer.sh
  rm run_trainer.sh
}

function deploy_cronjob() {
  RESULT=$(kubectl get secret --ignore-not-found=true twitter-oauth)
  if [ -z "$RESULT" ]; then
  echo "Creating Twitter Secrets..."
    kubectl create secret generic twitter-oauth \
      --from-literal=TWITTER_CONSUMER_KEY=$TWITTER_CONSUMER_KEY \
      --from-literal=TWITTER_CONSUMER_SECRET=$TWITTER_CONSUMER_SECRET \
      --from-literal=TWITTER_ACCESS_TOKEN=$TWITTER_ACCESS_TOKEN \
      --from-literal=TWITTER_TOKEN_SECRET=$TWITTER_TOKEN_SECRET
  fi
  echo "Deploying Twitter CronJob..."
  envsubst < twitter-poster/cronjob.yml | kubectl apply -f -
}

case $ACTION in

  create)
    case $RESOURCE in
      archive)
        echo "Creating torchserve archive..."
        download_model
        (cd serve-handler/src && archive)
        ;;
      corpus)
        echo "Creating corpus from dataset..."
        build_corpus
        ;;
      trainer)
        echo "Creating training VM..."
        deploy_trainer
        ;;
      cronjob)
        echo "Creating twitter cronjob..."
        deploy_cronjob
        ;;
      report)
        echo "Creating generated sentences and report..."
        build_sentence_report
        ;;
      image)
        echo "Creating twitter poster image..."
        (cd twitter-poster && docker build -t $TWITTER_IMAGE_NAME .)
        ;;
      token)
        echo "Fetching twitter access token..."
        poetry run python twitter-poster/twitter/main.py token
        ;;
      *)
        echo "Resource not understood."
        ;;
    esac
    ;;
  upload)
    case $RESOURCE in
      archive)
        echo "Uploading torchserve assets..."
        upload_handler_archive
        ;;
      corpus)
        echo "Uploading corpus dataset..."
        upload_corpus
        ;;
      image)
        echo "Uploading twitter poster image..."
        docker push $TWITTER_IMAGE_NAME
        ;;
      *)
        echo "Resource not understood."
        ;;
    esac
    ;;
  download)
    case $RESOURCE in
      archive)
        echo "Downloading torchserve assets..."
        download_handler_archive
        ;;
      corpus)
        echo "Downloading corpus dataset..."
        download_corpus
        ;;
      image)
        echo "Downloading twitter poster image..."
        docker pull $TWITTER_IMAGE_NAME
        ;;
      model)
        echo "Downloading soulsgen GPT2 model..."
        download_model
        ;;
      *)
        echo "Resource not understood."
        ;;
    esac
    ;;
  run)
    echo "Running torchserve instance..."
    docker run --rm -it -d -p 8080:8080 -p 8081:8081 -p 8082:8082 -p 7070:7070 -p 7071:7071 \
        -v ${PWD}/serve-handler:/home/model-server/mnt --name serve \
        -e GPT2_VOCAB_PATH=/home/model-server/mnt/assets/vocab.txt \
        -e GPT2_CORPUS_PATH=/home/model-server/mnt/assets/corpus.txt \
        pytorch/torchserve:latest-cpu \
        torchserve --start --ts-config /home/model-server/config.properties --model-store=/home/model-server/mnt/ --models=soulsgen.mar
    echo "Log torchserve instance 'docker logs serve'"
    echo "Test it when ready 'curl -X POST http://127.0.0.1:8080/predictions/soulsgen'"
    ;;

  clean)
    echo "Removing txt files, pth files, mar files and the base expanda config..."
    rm -rf serve-handler/**/*.txt
    rm -rf dataset/**/*.txt
    rm -rf dataset/build/*.html
    rm -rf **/*.pth
    rm -rf **/*.mar
    rm -rf $EXPANDA_CONFIG
    ;;

  install)
    echo "Installing python deps..."
    # Weird quirk with cython and poetry, doesn't build deps correctly
    # install what we can, manually install cython in pip and retry will work
    # https://github.com/python-poetry/poetry/issues/2789
    poetry install || true
    poetry run pip install Cython
    poetry install
    # Pull spacy eng parsing model data
    poetry run python -m spacy download en_core_web_sm
    ;;
  *)
    echo "Command not understood."
    ;;
esac
